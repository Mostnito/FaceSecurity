#import library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
import streamlit as st
import cv2
import numpy as np
import os
import random
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf

st.title("Face Security")

frame_placeholder = st.empty() #‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡πâ‡∏≠‡∏á



if "Mode" not in st.session_state: #‡∏Å‡∏≥‡∏´‡∏ô‡∏îstate ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÑ‡∏ß‡πâ
    st.session_state.Mode = "menu"
if "run_camera" not in st.session_state:
    st.session_state.run_camera = False
if "member" not in st.session_state:
    st.session_state.member = False
if "train" not in st.session_state:
    st.session_state.train = False
if "model" not in st.session_state:
    st.session_state.model = None
if "class_names" not in st.session_state:
    st.session_state.class_names = None


# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

if st.session_state.Mode == "menu":
    col1,col2,col3 = st.columns(3)
    
    SetupMode_btn = col1.button("Setup",width="stretch") #‡∏õ‡∏∏‡πà‡∏°setup
    Start_btn = col2.button("Start",width="stretch") #‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á
    Stop_btn = col3.button("Stop Camera",width="stretch")#‡∏õ‡∏∏‡πà‡∏°‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á


    if SetupMode_btn: #‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Setup
        st.session_state.Mode = "setup"
        st.session_state.run_camera = False
        st.rerun()
    if Start_btn: #‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Start
        st.session_state.run_camera = True
    if Stop_btn: #‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Stop
        st.session_state.run_camera = False
        
elif st.session_state.Mode == "setup":
    #Setup Mode
    if st.button("‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö", width="stretch"): #‡∏õ‡∏∏‡πà‡∏° ‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å
        st.session_state.Mode = "menu"
        st.rerun()

    st.markdown("<h1 style='text-align: center; color: grey;'>‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡πÉ‡∏ô‡∏ö‡πâ‡∏≤‡∏ô</h1>", unsafe_allow_html=True) #title ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
    #‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏ÑFolder‡πÉ‡∏ôDataset
    folder_name = st.text_input("‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà:", placeholder="‡πÄ‡∏ä‡πà‡∏ô Most")
    if st.button("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"):
        if folder_name.strip() == "":
            st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")
        else:
            new_folder_path = os.path.join("DatasetTrain", folder_name.strip())
            if not os.path.exists(new_folder_path):
                os.makedirs(new_folder_path)
                st.success(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")
            else:
                st.info(f"‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_name} ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
    #‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏ÅDataSet
    existing_folders = [f for f in os.listdir("DatasetTrain") if os.path.isdir(os.path.join("DatasetTrain", f))]
    if existing_folders:
        selected_folder = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å Dataset:", existing_folders)
        st.write(f"‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏∑‡∏≠: {selected_folder}")
    else:
        st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å")
    #‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤", width="stretch",type="primary"):
        st.session_state.member = True
        st.rerun()

    st.markdown("<h1 style='text-align: center; color: grey;'>‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•</h1>", unsafe_allow_html=True) #title ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
    if not os.path.exists("DataSetTrain"):
        st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    else:
        # ‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô DatasetTrain
        folders = [f for f in os.listdir("DataSetTrain") if os.path.isdir(os.path.join("DataSetTrain", f))]

        if len(folders) == 0:
            st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
        else:
            st.write(f"‡∏û‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(folders)} ‡∏Ñ‡∏ô")

            for folder in folders:
                folder_path = os.path.join("DataSetTrain", folder)
                images = [f for f in os.listdir(folder_path)
                        if f.lower().endswith((".png"))]

                st.subheader(f"‡∏Ñ‡∏∏‡∏ì {folder}")
                if len(images) == 0:
                    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ")
                else:
                    random_img = random.choice(images)
                    img_path = os.path.join(folder_path, random_img)
                    st.image(Image.open(img_path), caption=f"{folder} - {random_img}", width=200)
    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•", width="stretch",type="primary"): #‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        st.session_state.train = True

#‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°state‡∏ô‡∏µ‡πâ
if st.session_state.run_camera:
    cam = cv2.VideoCapture(0)
    class_names = st.session_state.class_names
    print(class_names)
    while st.session_state.run_camera:
        ret, frame = cam.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        #‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û scaleFactor
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        if len(faces) > 0:
            for (x, y, w, h) in faces:
                face_img = frame[y:y+h, x:x+w]
                face_img = cv2.resize(face_img, (150, 150)) / 255.0
                face_img = np.expand_dims(face_img, axis=0)
                if st.session_state.model is None:
                    label = "Please Setup"
                    color = (0,0,255)
                else:
                    preds = st.session_state.model.predict(face_img)
                    idx = np.argmax(preds)#‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á0,1,2
                    conf = np.max(preds)#‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                    if conf >= 0.8: #‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà0.8
                        label = f"Detect:  {class_names[idx]} ({conf*100:.1f}%)" #‡∏û‡∏ö‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£ ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Å‡∏µ‡πà%
                        color = (0, 255, 0) #‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
                    else:
                        label = f"Unknow ({conf*100:.1f}%)" #‡∏û‡∏ö‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
                        color = (0, 0, 255) #‡∏™‡∏µ‡πÅ‡∏î‡∏á
                # ‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤
                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            cv2.putText(frame, "No face detected", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        frame_placeholder.image(frame, channels="RGB")
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cam.release()
    cv2.destroyAllWindows()
    print("‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

#‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°state‡∏ô‡∏µ‡πâ
if st.session_state.member:
    cam = cv2.VideoCapture(0)
    i=1
    dataset_dir = os.path.join(os.getcwd(), f"DatasetTrain/{selected_folder}")
    while st.session_state.member:
        ret, frame = cam.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        #‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û scaleFactor
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))


        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        if len(faces) > 0:
            for (x, y, w, h) in faces:
                face_img = frame[y:y+h, x:x+w]
                face_img = cv2.resize(face_img, (150, 150)) / 255.0
                face_img = np.expand_dims(face_img, axis=0)

                filename = f"{selected_folder}_{i}.png"
                filepath = os.path.join(dataset_dir, filename)

                reface_img = frame[y:y+h, x:x+w]
                reface_img = cv2.resize(reface_img, (150, 150))

                cv2.imwrite(filepath, cv2.cvtColor(reface_img, cv2.COLOR_RGB2BGR))
                i+=1
                label = f"Please Wait"
                if i>=800:
                    label = f"Look At Camera {i}"
                elif i>=700:
                    label = f"Please Smile {i}"
                elif i>=600:
                    label = f"Open Your Mouth {i}"
                elif i>= 450:
                    label = f"Blink Your Eyes {i}"
                elif i>= 300:
                    label = f"Turn Left Slowly {i}"
                elif i>= 200:
                    label = f"Turn Right Slowly {i}"
                elif i>= 100:
                    label = f"Nod Your Head {i}"
                else:
                    label = f"Look At Camera {i}"
                color = (0,255,0)
                # ‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤
                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            cv2.putText(frame, "No face detected", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        frame_placeholder.image(frame, channels="RGB")
        if cv2.waitKey(1) & 0xFF == ord('q') or i > 1000:
            break
    cam.release()
    cv2.destroyAllWindows()
    frame_placeholder = st.empty()
    st.session_state.member = False
    print("‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    st.rerun()

#‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°state‡∏ô‡∏µ‡πâ
if st.session_state.train:
    dataset_dir = os.path.join(os.getcwd(), f"DatasetTrain")
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    datagen = ImageDataGenerator(
        rescale=1./255,
        validation_split=0.2,
        rotation_range=20,
        zoom_range=0.2,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.2,
        horizontal_flip=True,
    )
    BATCH_SIZE = 32
    IMG_SIZE = 150


    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å (Train)
    train_dataset = datagen.flow_from_directory(
        dataset_dir,
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical',
        subset='training'
    )

    val_dataset = datagen.flow_from_directory(
        dataset_dir,
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical',
        subset='validation'
    )

    base_model = tf.keras.applications.MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet'
    )

    base_model.trainable = False

    class_names = list(train_dataset.class_indices.keys())
    print("‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏û‡∏ö:", class_names)

    model = tf.keras.Sequential([ 
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(3, activation='softmax')
    ])

    model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
    EPOCHS = 5
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà . . .")
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    for epoch in range(EPOCHS):
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=1, 
        )
        train_acc = history.history['accuracy'][0]
        val_acc = history.history['val_accuracy'][0]
        loss = history.history['loss'][0]
        progress = int(((epoch + 1) / EPOCHS) * 100)
        progress_bar.progress(progress)
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å Epoch {epoch+1}/{EPOCHS} " f"‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ {train_acc:.2f} | Validation {val_acc:.2f} | Loss {loss:.2f}")
    st.session_state.model = model
    st.session_state.class_names = class_names
    st.success("üéâ ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    st.session_state.train = False
