import streamlit as st
import cv2
import numpy as np
import os
import random
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tensorflow as tf

st.title("Face Security")

frame_placeholder = st.empty()



if "Mode" not in st.session_state:
    st.session_state.Mode = "menu"
if "run_camera" not in st.session_state:
    st.session_state.run_camera = False
if "member" not in st.session_state:
    st.session_state.member = False
if "train" not in st.session_state:
    st.session_state.train = False
if "model" not in st.session_state:
    st.session_state.model = None
if "class_names" not in st.session_state:
    st.session_state.class_names = None


# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

if st.session_state.Mode == "menu":
    col1,col2,col3 = st.columns(3)
    
    SetupMode_btn = col1.button("Setup",width="stretch")
    Start_btn = col2.button("Start",width="stretch")
    Stop_btn = col3.button("Stop Camera",width="stretch")


    if SetupMode_btn:
        st.session_state.Mode = "setup"
        st.session_state.run_camera = False
        st.rerun()
    if Start_btn:
        st.session_state.run_camera = True
    if Stop_btn:
        st.session_state.run_camera = False
        
elif st.session_state.Mode == "setup":
    #Setup Mode
    

    if st.button("‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö", width="stretch"):
        st.session_state.Mode = "menu"
        st.rerun()

    st.markdown("<h1 style='text-align: center; color: grey;'>‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡πÉ‡∏ô‡∏ö‡πâ‡∏≤‡∏ô</h1>", unsafe_allow_html=True)   

    folder_name = st.text_input("‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà:", placeholder="‡πÄ‡∏ä‡πà‡∏ô Most")
    if st.button("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"):
        if folder_name.strip() == "":
            st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")
        else:
            new_folder_path = os.path.join("DatasetTrain", folder_name.strip())
            if not os.path.exists(new_folder_path):
                os.makedirs(new_folder_path)
                st.success(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")
            else:
                st.info(f"‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {folder_name} ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")

    existing_folders = [f for f in os.listdir("DatasetTrain") if os.path.isdir(os.path.join("DatasetTrain", f))]
    if existing_folders:
        selected_folder = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å Dataset:", existing_folders)
        st.write(f"‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏∑‡∏≠: {selected_folder}")
    else:
        st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å")

    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤", width="stretch",type="primary"):
        st.session_state.member = True
        st.rerun()

    st.markdown("<h1 style='text-align: center; color: grey;'>‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•</h1>", unsafe_allow_html=True)
    if not os.path.exists("DataSetTrain"):
        st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    else:
        # ‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô DatasetTrain
        folders = [f for f in os.listdir("DataSetTrain") if os.path.isdir(os.path.join("DataSetTrain", f))]

        if len(folders) == 0:
            st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
        else:
            st.write(f"‡∏û‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(folders)} ‡∏Ñ‡∏ô")

            for folder in folders:
                folder_path = os.path.join("DataSetTrain", folder)
                images = [f for f in os.listdir(folder_path)
                        if f.lower().endswith((".png"))]

                st.subheader(f"‡∏Ñ‡∏∏‡∏ì {folder}")
                if len(images) == 0:
                    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ")
                else:
                    random_img = random.choice(images)
                    img_path = os.path.join(folder_path, random_img)
                    st.image(Image.open(img_path), caption=f"{folder} - {random_img}", width=200)
    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•", width="stretch",type="primary"):
        st.session_state.train = True

#Detection Mode
if st.session_state.run_camera:
    cam = cv2.VideoCapture(0)
    class_names = st.session_state.class_names
    print(class_names)
    while st.session_state.run_camera:
        ret, frame = cam.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        #‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û scaleFactor
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))


        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        if len(faces) > 0:
            for (x, y, w, h) in faces:
                face_img = frame[y:y+h, x:x+w]
                face_img = cv2.resize(face_img, (150, 150)) / 255.0
                face_img = np.expand_dims(face_img, axis=0)
                if st.session_state.model is None:
                    label = "Please Setup"
                    color = (0,0,255)
                else:
                    preds = st.session_state.model.predict(face_img)
                    idx = np.argmax(preds)#‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á0,1,2
                    conf = np.max(preds)#‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                    if conf >= 0.8: #‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ó‡∏µ‡πà0.8
                        label = f"Detect:  {class_names[idx]} ({conf*100:.1f}%)" #‡∏û‡∏ö‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£ ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Å‡∏µ‡πà%
                        color = (0, 255, 0) #‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
                    else:
                        label = f"Unknow ({conf*100:.1f}%)" #‡∏û‡∏ö‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
                        color = (0, 0, 255) #‡∏™‡∏µ‡πÅ‡∏î‡∏á
                # ‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤
                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            cv2.putText(frame, "No face detected", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        frame_placeholder.image(frame, channels="RGB")
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cam.release()
    cv2.destroyAllWindows()
    print("‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")


if st.session_state.member:
    cam = cv2.VideoCapture(0)
    i=1
    dataset_dir = os.path.join(os.getcwd(), f"DatasetTrain/{selected_folder}")
    while st.session_state.member:
        ret, frame = cam.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        #‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û scaleFactor
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))


        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        if len(faces) > 0:
            for (x, y, w, h) in faces:
                face_img = frame[y:y+h, x:x+w]
                face_img = cv2.resize(face_img, (150, 150)) / 255.0
                face_img = np.expand_dims(face_img, axis=0)

                filename = f"{selected_folder}_{i}.png"
                filepath = os.path.join(dataset_dir, filename)

                reface_img = frame[y:y+h, x:x+w]
                reface_img = cv2.resize(reface_img, (150, 150))

                cv2.imwrite(filepath, cv2.cvtColor(reface_img, cv2.COLOR_RGB2BGR))
                i+=1
                label = f"Please Wait"
                if i>=800:
                    label = f"Look At Camera {i}"
                elif i>=700:
                    label = f"Please Smile {i}"
                elif i>=600:
                    label = f"Open Your Mouth {i}"
                elif i>= 450:
                    label = f"Blink Your Eyes {i}"
                elif i>= 300:
                    label = f"Turn Left Slowly {i}"
                elif i>= 200:
                    label = f"Turn Right Slowly {i}"
                elif i>= 100:
                    label = f"Payak Naa Cha Cha {i}"
                else:
                    label = f"Look At Camera {i}"
                color = (0,255,0)
                # ‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤
                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            cv2.putText(frame, "No face detected", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        frame_placeholder.image(frame, channels="RGB")
        if cv2.waitKey(1) & 0xFF == ord('q') or i > 1000:
            break
    cam.release()
    cv2.destroyAllWindows()
    frame_placeholder = st.empty()
    st.session_state.member = False
    print("‡∏õ‡∏¥‡∏î‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
    st.rerun()

if st.session_state.train:
    dataset_dir = os.path.join(os.getcwd(), f"DatasetTrain")
    from tensorflow.keras.preprocessing.image import ImageDataGenerator #‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ImageDataGenerator‡∏à‡∏≤‡∏Å Keras ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏≠‡∏ö‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û
    datagen = ImageDataGenerator(
        rescale=1./255, #‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏™‡∏µ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏ä‡πà‡∏ß‡∏á [0, 255] ‡πÑ‡∏õ [0, 1]
        validation_split=0.2, #‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏övalidation 20%
        rotation_range=20, #‡∏ã‡∏π‡∏°‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 20‡∏≠‡∏á‡∏®‡∏≤ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
        zoom_range=0.2, #‡∏£‡∏∞‡∏¢‡∏∞‡∏ã‡∏π‡∏°‡∏†‡∏≤‡∏û‡∏™‡∏∏‡πà‡∏°20% ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        width_shift_range=0.1, #‡∏Ç‡∏¢‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 10% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á
        height_shift_range=0.1, #‡∏Ç‡∏¢‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 10% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡∏á
        shear_range=0.2, #‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ ‡πÄ‡∏â‡∏∑‡∏≠‡∏ô (shear transformation) ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
        horizontal_flip=True, #‡∏Å‡∏•‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô (‡∏ã‡πâ‡∏≤‡∏¢‚Üî‡∏Ç‡∏ß‡∏≤) ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°
    )
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    BATCH_SIZE = 32
    IMG_SIZE = 150 # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏Å‡∏ß‡πâ‡∏≤‡∏á x ‡∏™‡∏π‡∏á)


    #datagen.flow_from_directory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö batch
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å (Train)
    train_dataset = datagen.flow_from_directory(
        dataset_dir,#path folder ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡πÄ‡∏ó‡∏£‡∏ô
        target_size=(150, 150),#‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û resize ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î 150√ó150 pixel ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•
        batch_size=32, #‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏à‡∏∞‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞ 32 ‡∏†‡∏≤‡∏û ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏¥‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        class_mode='categorical', #‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ label ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö ‚Äú‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™‚Äù (multi-class)
        subset='training'#‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô training
    )

    val_dataset = datagen.flow_from_directory(
        dataset_dir,
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical',
        subset='validation' #‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô validation
    )

    base_model = tf.keras.applications.MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),#‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡πâ‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• 3 ‡∏Ñ‡∏∑‡∏≠ RGB
        include_top=False,#‚Üí ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏™‡πà‡∏ß‡∏ô ‚Äú‡∏´‡∏±‡∏ß‡πÇ‡∏°‡πÄ‡∏î‡∏•‚Äù (fully connected layers) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Äúhead‚Äù ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô dataset ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á
        weights='imagenet' #‡πÇ‡∏´‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤‡∏à‡∏≤‡∏Å ImageNet ‡∏°‡∏≤‡πÉ‡∏ä‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å feature ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß(transfer learning)
    )

    base_model.trainable = False  # ‡πÑ‡∏°‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏ä‡∏±‡πâ‡∏ô base

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ ('Aom','Most','New')
    class_names = list(train_dataset.class_indices.keys())
    print("‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏û‡∏ö:", class_names)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô (Sequential)
    # Sequential: ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πâ‡∏ô (layer) ‡∏°‡∏≤‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö
    model = tf.keras.Sequential([ 
        base_model, #‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ê‡∏≤‡∏ô ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ (pretrained model)
    tf.keras.layers.GlobalAveragePooling2D(), #‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Feature Map ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å base_model
        tf.keras.layers.Flatten(),#‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å2D,3D ‡πÄ‡∏õ‡πá‡∏ô 1D
        tf.keras.layers.Dense(128, activation='relu'), #‡πÉ‡∏ä‡πârelu‡∏õ‡∏£‡∏±‡∏ö
        tf.keras.layers.Dropout(0.3),#‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏≤‡∏á neuron ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° 30% (0.3) ‡∏Ç‡∏ì‡∏∞‡πÄ‡∏ó‡∏£‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ôoverfitting
        tf.keras.layers.Dense(3, activation='softmax')  # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å3class ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Softmax ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ 1
    ])

        # optimizer (‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•), loss (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î) ‡πÅ‡∏•‡∏∞ metrics (‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡πÄ‡∏ä‡πà‡∏ô accuracy ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
    model.compile(optimizer='adam', #‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô deep learning
                loss='categorical_crossentropy', #‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™
                metrics=['accuracy'])
    EPOCHS = 1 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
    progress_bar = st.progress(0)
    status_text = st.empty()
    status_text.text("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà . . .")
    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    for epoch in range(EPOCHS):
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=1,  # ‡∏ù‡∏∂‡∏Å‡∏ó‡∏µ‡∏•‡∏∞ 1 epoch
        )
        train_acc = history.history['accuracy'][0]
        val_acc = history.history['val_accuracy'][0]
        loss = history.history['loss'][0]
        progress = int(((epoch + 1) / EPOCHS) * 100)
        progress_bar.progress(progress)
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å Epoch {epoch+1}/{EPOCHS} " f"‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ {train_acc:.2f} | Validation {val_acc:.2f} | Loss {loss:.2f}")
    st.session_state.model = model
    st.session_state.class_names = class_names
    st.success("üéâ ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    st.session_state.train = False